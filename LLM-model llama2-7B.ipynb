{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d3938c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/noarch::conda-pack==0.6.0=pyhd3eb1b0_0\n",
      "  - defaults/osx-arm64::huggingface_hub==0.10.1=py310hca03da5_0\n",
      "  - defaults/osx-arm64::zope.interface==5.4.0=py310h1a28f6b_0\n",
      "  - defaults/osx-arm64::nbformat==5.7.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::panel==0.14.3=py310hca03da5_0\n",
      "  - defaults/osx-arm64::black==22.6.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::anaconda-project==0.11.1=py310hca03da5_0\n",
      "  - defaults/osx-arm64::conda-repo-cli==1.0.41=py310hca03da5_0\n",
      "  - defaults/osx-arm64::qtconsole==5.4.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::python-lsp-black==1.2.1=py310hca03da5_0\n",
      "  - defaults/osx-arm64::nbclassic==0.5.2=py310hca03da5_0\n",
      "  - defaults/osx-arm64::datashader==0.14.4=py310hca03da5_0\n",
      "  - defaults/osx-arm64::anaconda-navigator==2.4.0=py310hca03da5_0\n",
      "  - defaults/noarch::argon2-cffi==21.3.0=pyhd3eb1b0_0\n",
      "  - defaults/osx-arm64::jupyterlab_server==2.19.0=py310hca03da5_0\n",
      "  - defaults/noarch::backports.functools_lru_cache==1.6.4=pyhd3eb1b0_0\n",
      "  - defaults/osx-arm64::jupyter_server==1.23.4=py310hca03da5_0\n",
      "  - defaults/osx-arm64::python-lsp-server==1.7.1=py310hca03da5_0\n",
      "  - defaults/osx-arm64::hvplot==0.8.2=py310hca03da5_0\n",
      "  - defaults/osx-arm64::notebook-shim==0.2.2=py310hca03da5_0\n",
      "  - defaults/noarch::pyls-spyder==0.4.0=pyhd3eb1b0_0\n",
      "  - defaults/osx-arm64::bokeh==2.4.3=py310hca03da5_0\n",
      "  - defaults/osx-arm64::jupyterlab==3.5.3=py310hca03da5_0\n",
      "  - defaults/osx-arm64::jupyter_client==7.3.4=py310hca03da5_0\n",
      "  - defaults/osx-arm64::anaconda-client==1.11.2=py310hca03da5_0\n",
      "  - defaults/osx-arm64::nbconvert==6.5.4=py310hca03da5_0\n",
      "  - defaults/osx-arm64::pylint==2.16.2=py310hca03da5_0\n",
      "  - defaults/osx-arm64::jupyter_core==5.2.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::dask==2022.7.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::astroid==2.14.2=py310hca03da5_0\n",
      "  - defaults/osx-arm64::pytoolconfig==1.2.5=py310hca03da5_1\n",
      "  - defaults/osx-arm64::ipykernel==6.19.2=py310h33ce5c2_0\n",
      "  - defaults/osx-arm64::scrapy==2.8.0=py310hca03da5_0\n",
      "  - defaults/noarch::conda-verify==3.4.2=py_1\n",
      "  - defaults/osx-arm64::twisted==22.2.0=py310h1a28f6b_1\n",
      "  - defaults/osx-arm64::numba==0.56.4=py310h46d7db6_0\n",
      "  - defaults/osx-arm64::conda-build==3.24.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::nbclient==0.5.13=py310hca03da5_0\n",
      "  - defaults/osx-arm64::bcrypt==3.2.0=py310h1a28f6b_1\n",
      "  - defaults/osx-arm64::holoviews==1.15.4=py310hca03da5_0\n",
      "  - defaults/osx-arm64::conda==23.3.1=py310hca03da5_0\n",
      "  - defaults/osx-arm64::transformers==4.24.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::spyder==5.4.1=py310hca03da5_0\n",
      "  - defaults/osx-arm64::navigator-updater==0.3.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::notebook==6.5.2=py310hca03da5_0\n",
      "  - defaults/osx-arm64::typing-extensions==4.4.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::intake==0.6.7=py310hca03da5_0\n",
      "  - defaults/osx-arm64::distributed==2022.7.0=py310hca03da5_0\n",
      "  - defaults/noarch::conda-token==0.4.0=pyhd3eb1b0_0\n",
      "  - defaults/osx-arm64::clyent==1.2.2=py310hca03da5_1\n",
      "  - defaults/osx-arm64::rope==1.7.0=py310hca03da5_0\n",
      "  - defaults/osx-arm64::spyder-kernels==2.4.1=py310hca03da5_0\n",
      "  - defaults/osx-arm64::pytorch==1.12.1=cpu_py310h8370978_1\n",
      "  - defaults/noarch::tldextract==3.2.0=pyhd3eb1b0_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/shirbaz/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  brotli-python      pkgs/main/osx-arm64::brotli-python-1.0.9-py310hc377ac9_7 \n",
      "  cyrus-sasl         pkgs/main/osx-arm64::cyrus-sasl-2.1.28-h9131b1a_1 \n",
      "  exceptiongroup     pkgs/main/osx-arm64::exceptiongroup-1.0.4-py310hca03da5_0 \n",
      "  filelock           pkgs/main/osx-arm64::filelock-3.13.1-py310hca03da5_0 \n",
      "  jaraco.classes     pkgs/main/noarch::jaraco.classes-3.2.1-pyhd3eb1b0_0 \n",
      "  jsonschema-specif~ pkgs/main/osx-arm64::jsonschema-specifications-2023.7.1-py310hca03da5_0 \n",
      "  libclang13         pkgs/main/osx-arm64::libclang13-14.0.6-default_h24352ff_1 \n",
      "  markdown-it-py     pkgs/main/osx-arm64::markdown-it-py-2.2.0-py310hca03da5_1 \n",
      "  mdurl              pkgs/main/osx-arm64::mdurl-0.1.0-py310hca03da5_0 \n",
      "  more-itertools     pkgs/main/osx-arm64::more-itertools-10.1.0-py310hca03da5_0 \n",
      "  mysql              pkgs/main/osx-arm64::mysql-5.7.24-ha71a6ea_2 \n",
      "  pip                pkgs/main/osx-arm64::pip-23.3.1-py310hca03da5_0 \n",
      "  platformdirs       pkgs/main/osx-arm64::platformdirs-3.10.0-py310hca03da5_0 \n",
      "  python-lmdb        pkgs/main/osx-arm64::python-lmdb-1.4.1-py310h313beb8_0 \n",
      "  python-tzdata      pkgs/main/noarch::python-tzdata-2023.3-pyhd3eb1b0_0 \n",
      "  referencing        pkgs/main/osx-arm64::referencing-0.30.2-py310hca03da5_0 \n",
      "  rich               pkgs/main/osx-arm64::rich-13.3.5-py310hca03da5_0 \n",
      "  rpds-py            pkgs/main/osx-arm64::rpds-py-0.10.6-py310hf0e4da2_0 \n",
      "  setuptools         pkgs/main/osx-arm64::setuptools-68.0.0-py310hca03da5_0 \n",
      "  typing_extensions  pkgs/main/osx-arm64::typing_extensions-4.7.1-py310hca03da5_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  attrs                              22.1.0-py310hca03da5_0 --> 23.1.0-py310hca03da5_0 \n",
      "  beautifulsoup4                     4.11.1-py310hca03da5_0 --> 4.12.2-py310hca03da5_0 \n",
      "  ca-certificates                     2023.01.10-hca03da5_0 --> 2023.08.22-hca03da5_0 \n",
      "  certifi                         2022.12.7-py310hca03da5_0 --> 2023.11.17-py310hca03da5_0 \n",
      "  cffi                               1.15.1-py310h80987f9_3 --> 1.16.0-py310h80987f9_0 \n",
      "  click                               8.0.4-py310hca03da5_0 --> 8.1.7-py310hca03da5_0 \n",
      "  cloudpickle        pkgs/main/noarch::cloudpickle-2.0.0-p~ --> pkgs/main/osx-arm64::cloudpickle-2.2.1-py310hca03da5_0 \n",
      "  contourpy                           1.0.5-py310h525c30c_0 --> 1.2.0-py310h48ca7d4_0 \n",
      "  cookiecutter       pkgs/main/noarch::cookiecutter-1.7.3-~ --> pkgs/main/osx-arm64::cookiecutter-2.5.0-py310hca03da5_0 \n",
      "  cryptography                       39.0.1-py310h834c97f_0 --> 41.0.3-py310hd4332d6_0 \n",
      "  debugpy                             1.5.1-py310hc377ac9_0 --> 1.6.7-py310h313beb8_0 \n",
      "  dill                                0.3.6-py310hca03da5_0 --> 0.3.7-py310hca03da5_0 \n",
      "  fsspec                          2022.11.0-py310hca03da5_0 --> 2023.10.0-py310hca03da5_0 \n",
      "  gettext                                 0.21.0-h826f4ad_0 --> 0.21.0-h13f89a0_1 \n",
      "  icu                                       68.1-hc377ac9_0 --> 73.1-h313beb8_0 \n",
      "  importlib-metadata                 4.11.3-py310hca03da5_0 --> 6.0.0-py310hca03da5_0 \n",
      "  importlib_metadata                      4.11.3-hd3eb1b0_0 --> 6.0.0-hd3eb1b0_0 \n",
      "  ipython                            8.10.0-py310hca03da5_0 --> 8.15.0-py310hca03da5_0 \n",
      "  jellyfish                           0.9.0-py310h1a28f6b_0 --> 1.0.1-py310h15d1925_0 \n",
      "  jmespath           pkgs/main/noarch::jmespath-0.10.0-pyh~ --> pkgs/main/osx-arm64::jmespath-1.0.1-py310hca03da5_0 \n",
      "  jsonschema                         4.17.3-py310hca03da5_0 --> 4.19.2-py310hca03da5_0 \n",
      "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments~ --> pkgs/main/osx-arm64::jupyterlab_pygments-0.2.2-py310hca03da5_0 \n",
      "  keyring                            23.4.0-py310hca03da5_0 --> 23.13.1-py310hca03da5_0 \n",
      "  krb5                                    1.19.4-h8380606_0 --> 1.20.1-hf3e1bf2_1 \n",
      "  ldid                                     2.1.2-h64d1936_2 --> 2.1.5-h20b2a84_3 \n",
      "  libarchive                               3.6.2-h3d4dd14_0 --> 3.6.2-h62fee54_2 \n",
      "  libclang                        12.0.0-default_hc321e17_4 --> 14.0.6-default_h1b80db6_1 \n",
      "  libdeflate                                1.17-h80987f9_0 --> 1.17-h80987f9_1 \n",
      "  libffi                                   3.4.2-hca03da5_6 --> 3.4.4-hca03da5_0 \n",
      "  libllvm14                               14.0.6-h7ec7a93_2 --> 14.0.6-h7ec7a93_3 \n",
      "  libpq                                     12.9-h65cfe13_3 --> 12.15-h02f6b3c_1 \n",
      "  libtiff                                  4.5.0-h313beb8_2 --> 4.5.1-h313beb8_0 \n",
      "  libwebp                                  1.2.4-ha3663a8_1 --> 1.3.2-ha3663a8_0 \n",
      "  libwebp-base                             1.2.4-h80987f9_1 --> 1.3.2-h80987f9_0 \n",
      "  libxml2                                 2.9.14-h8c5e841_0 --> 2.10.4-h0dcf63f_1 \n",
      "  libxslt                                 1.1.35-h9833966_0 --> 1.1.37-h80987f9_1 \n",
      "  lxml                                4.9.1-py310h2fae87d_0 --> 4.9.3-py310h50ffb84_0 \n",
      "  lz4                                 3.1.3-py310h1a28f6b_0 --> 4.3.2-py310h80987f9_0 \n",
      "  matplotlib-base                     3.7.0-py310h46d7db6_0 --> 3.8.0-py310h46d7db6_0 \n",
      "  mypy_extensions                     0.4.3-py310hca03da5_1 --> 1.0.0-py310hca03da5_0 \n",
      "  numexpr                             2.8.4-py310hecc3335_0 --> 2.8.7-py310hecc3335_0 \n",
      "  openssl                                 1.1.1t-h1a28f6b_0 --> 3.0.12-h1a28f6b_0 \n",
      "  packaging                            22.0-py310hca03da5_0 --> 23.1-py310hca03da5_0 \n",
      "  pandas                              1.5.3-py310h46d7db6_0 --> 2.1.1-py310h46d7db6_0 \n",
      "  param                              1.12.3-py310hca03da5_0 --> 1.13.0-py310hca03da5_0 \n",
      "  partd              pkgs/main/noarch::partd-1.2.0-pyhd3eb~ --> pkgs/main/osx-arm64::partd-1.4.1-py310hca03da5_0 \n",
      "  pillow                              9.4.0-py310h313beb8_0 --> 10.0.1-py310h3b245a6_0 \n",
      "  pycosat                             0.6.4-py310h1a28f6b_0 --> 0.6.6-py310h80987f9_0 \n",
      "  pygments           pkgs/main/noarch::pygments-2.11.2-pyh~ --> pkgs/main/osx-arm64::pygments-2.15.1-py310hca03da5_1 \n",
      "  pyopenssl                          23.0.0-py310hca03da5_0 --> 23.2.0-py310hca03da5_0 \n",
      "  pyqt                               5.15.7-py310hc377ac9_0 --> 5.15.10-py310h313beb8_0 \n",
      "  pyqt5-sip                         12.11.0-py310hc377ac9_0 --> 12.13.0-py310h80987f9_0 \n",
      "  pyqtwebengine                      5.15.7-py310hc377ac9_0 --> 5.15.10-py310h313beb8_0 \n",
      "  python                                  3.10.9-hc0d8a6c_1 --> 3.10.13-hb885b13_0 \n",
      "  pytz                               2022.7-py310hca03da5_0 --> 2023.3.post1-py310hca03da5_0 \n",
      "  pyviz_comms        pkgs/main/noarch::pyviz_comms-2.0.2-p~ --> pkgs/main/osx-arm64::pyviz_comms-2.3.0-py310hca03da5_0 \n",
      "  pyyaml                                6.0-py310h80987f9_1 --> 6.0.1-py310h80987f9_0 \n",
      "  pyzmq                              23.2.0-py310hc377ac9_0 --> 25.1.0-py310h313beb8_0 \n",
      "  qt-main                                 5.15.2-ha2d02b5_7 --> 5.15.2-h0917680_10 \n",
      "  qt-webengine                            5.15.9-h2903aaf_4 --> 5.15.9-h2903aaf_7 \n",
      "  qtpy                                2.2.0-py310hca03da5_0 --> 2.4.1-py310hca03da5_0 \n",
      "  queuelib                            1.5.0-py310hca03da5_0 --> 1.6.2-py310hca03da5_0 \n",
      "  regex                            2022.7.9-py310h1a28f6b_0 --> 2023.10.3-py310h80987f9_0 \n",
      "  requests                           2.28.1-py310hca03da5_0 --> 2.31.0-py310hca03da5_0 \n",
      "  requests-toolbelt  pkgs/main/noarch::requests-toolbelt-0~ --> pkgs/main/osx-arm64::requests-toolbelt-1.0.0-py310hca03da5_0 \n",
      "  scipy                              1.10.0-py310h20cbe94_1 --> 1.11.4-py310h20cbe94_0 \n",
      "  send2trash         pkgs/main/noarch::send2trash-1.8.0-py~ --> pkgs/main/osx-arm64::send2trash-1.8.2-py310hca03da5_0 \n",
      "  sip                                 6.6.2-py310hc377ac9_0 --> 6.7.12-py310h313beb8_0 \n",
      "  sleef                                    3.5.1-hf27765b_1 --> 3.5.1-h80987f9_2 \n",
      "  soupsieve                     2.3.2.post1-py310hca03da5_0 --> 2.5-py310hca03da5_0 \n",
      "  sqlite                                  3.40.1-h7a7dc30_0 --> 3.41.2-h80987f9_0 \n",
      "  tbb                                   2021.7.0-h48ca7d4_0 --> 2021.8.0-h48ca7d4_0 \n",
      "  tokenizers                         0.11.4-py310h5c5695e_1 --> 0.13.2-py310h3dd52b7_1 \n",
      "  tqdm                               4.64.1-py310hca03da5_0 --> 4.65.0-py310h33ce5c2_0 \n",
      "  tzdata                                   2022g-h04d1e81_0 --> 2023c-h04d1e81_0 \n",
      "  urllib3                           1.26.14-py310hca03da5_0 --> 1.26.18-py310hca03da5_0 \n",
      "  xarray                          2022.11.0-py310hca03da5_0 --> 2023.6.0-py310hca03da5_0 \n",
      "  xz                                      5.2.10-h80987f9_1 --> 5.4.2-h80987f9_0 \n",
      "  zict                                2.1.0-py310hca03da5_0 --> 3.0.0-py310hca03da5_0 \n",
      "  zstd                                     1.5.2-h8574219_0 --> 1.5.5-hd90d995_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: failed\n",
      "\n",
      "RemoveError: 'setuptools' is a dependency of conda and cannot be removed from\n",
      "conda's operating environment.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## These lines are comments that suggest potential conda commands for updating the base environment and installing necessary packages.\n",
    "#conda update -n base -c defaults conda\n",
    "#conda install -c conda-forge tqdm\n",
    "#conda install -c conda-forge jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d922cda",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Users/shirbaz/anaconda3/lib/python3.10/site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/shirbaz/anaconda3/lib/python3.10/site-packages (from wikipedia) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/shirbaz/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shirbaz/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shirbaz/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shirbaz/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/shirbaz/anaconda3/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=c3932ddb5e6b127f7a0b4fc9fed452a2b671498f50aaf76fb521f6a899492199\n",
      "  Stored in directory: /Users/shirbaz/Library/Caches/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## These are comments indicating potential pip commands for installing various Python packages like ctransformers, transformers, wikipedia-api, spacy, nltk, and downloading the English language model for spaCy.\n",
    "#!pip install ctransformers transformers\n",
    "# pip install wikipedia-api\n",
    "#!pip install spacy\n",
    "#!pip install nltk\n",
    "#!python -m spacy download en_core_web_sm\n",
    "# pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f427a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These lines import various libraries and modules that are used later in the code, including spacy, nltk, string, ctransformers, transformers, wikipediaapi, and wikipedia.\n",
    "# import spacy #tokenization #postagging\n",
    "# import nltk #tokenization\n",
    "# import string #tokenization\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize #tokenization\n",
    "# from ctransformers import AutoModelForCausalLM #postagging\n",
    "# from transformers import pipeline #postagging\n",
    "# import wikipediaapi\n",
    "# import wikipedia\n",
    "# from ctransformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31de3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This line loads the English language model for spaCy.\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0af8ca3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3366.22it/s]\n",
      "Fetching 1 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3187.16it/s]\n"
     ]
    }
   ],
   "source": [
    "## These lines import the AutoModelForCausalLM class from ctransformers and load a pre-trained language model named \"llama-2-7b.Q4_K_M.gguf.\"\n",
    "\n",
    "# repository = 'TheBloke/Llama-2-7B-GGUF'\n",
    "# model_file='llama-2-7b.Q4_K_M.gguf'\n",
    "# llm = AutoModelForCausalLM.from_pretrained(repository, model_file=model_file,model_type='llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76bd9b1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.6.attn.masked_bias', 'h.4.attn.masked_bias', 'h.9.attn.masked_bias', 'classifier.weight', 'h.3.attn.masked_bias', 'h.11.attn.masked_bias', 'h.8.attn.masked_bias', 'h.10.attn.masked_bias', 'classifier.bias', 'h.7.attn.masked_bias', 'h.2.attn.masked_bias', 'h.5.attn.masked_bias', 'h.1.attn.masked_bias', 'h.0.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## This line creates a part-of-speech tagging (POS tagging) pipeline using the GPT-2 model.\n",
    "## Load the POS tagging pipeline\n",
    "# pos_tagger = pipeline(task='token-classification', model='gpt2', tokenizer='gpt2', framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3e01cccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your question and type ENTER to finish: \n",
      "What is the capital city of germany?\n",
      "Completion: \n",
      " everyone knows that Berlin is Germany’s Capital, right? well not exactly. The German Parliament building was relocated from Berlin to Bonn in 1990 and remained there until reunification when it moved back to Berlin in 1999.\n",
      "Berlin has been the capital since the country’s founding in 1871 but for many years it wasn’t officially designated as such, instead being referred to as the “Prussian” or “Imperial” capital.\n",
      "The history of German government goes back much further than that though, and you can see a timeline of how that power evolved and then expanded from 1495 onwards in this infographic.\n",
      "It’s not just the seat of government that has moved around over time either – the country itself has grown so big over the centuries that it now consists of 16 states, each with their own regional parliament buildings as well.\n"
     ]
    }
   ],
   "source": [
    "## This section takes user input, passes it through the pre-trained language model (llm), and prints the generated completion.\n",
    "\n",
    "llm_input = input(\"Type your question and type ENTER to finish: \\n\")\n",
    "llm_output = llm(llm_input)\n",
    "print('Completion: %s' % llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "667bda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function defines a preprocessing step that involves removing punctuation, tokenizing the text into sentences and words.\n",
    "\n",
    "def preprocessing(text):\n",
    "    \n",
    "    start_index = text.find(\"\")\n",
    "    \n",
    "    # Skip the specified text and extract the relevant information\n",
    "    cleaned_text = text[start_index + len(\"\"):]\n",
    "   \n",
    "    # Remove punctuation\n",
    "    cleaned_text2 = cleaned_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Tokenization into sentences\n",
    "    sentences = sent_tokenize(cleaned_text2)\n",
    "\n",
    "    # Tokenization into words\n",
    "    tokenized_words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return tokenized_words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d4a3e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function performs part-of-speech tagging using spaCy.\n",
    "def pos_tagging(tokenized_words):\n",
    "    pos_tags = []\n",
    "    for sentence in tokenized_words:\n",
    "        # Join the words in the sentence into a string\n",
    "        sentence_text = \" \".join(sentence)\n",
    "        \n",
    "        # Apply spaCy's POS tagging\n",
    "        doc = nlp(sentence_text)\n",
    "        \n",
    "        # Extract POS tags for each word in the sentence\n",
    "        sentence_pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "        \n",
    "        pos_tags.append(sentence_pos_tags)\n",
    "\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "47d83d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function performs named entity recognition using spaCy.\n",
    "def ner(tokenized_words):\n",
    "    entities = []\n",
    "    for sentence in tokenized_words:\n",
    "        sentence_text = \" \".join(sentence)\n",
    "        doc = nlp(sentence_text)\n",
    "        sentence_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        entities.append(sentence_entities)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "b3feb8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function attempts to fetch the Wikipedia URL for a given entity.\n",
    "def fetch_wikipedia_url(entity):\n",
    "    try:\n",
    "        page = wikipedia.page(entity)\n",
    "        return page.url\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # Handle disambiguation pages if needed\n",
    "        print(f\"DisambiguationError: {e}\")\n",
    "        return None\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        # Handle page not found\n",
    "        print(f\"PageError: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2120c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation: These lines tokenize the input and output using the preprocessing function.\n",
    "tokenized_input = preprocessing(llm_input)\n",
    "tokenized_output = preprocessing(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1c3ac822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging: These lines perform part-of-speech tagging on the tokenized input and output.\n",
    "pos_tags_input = pos_tagging(tokenized_input)\n",
    "pos_tags_output = pos_tagging(tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f24c192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition: These lines perform named entity recognition on the tokenized input and output.\n",
    "entities_input = ner(tokenized_input)\n",
    "entities_output = ner(tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c7ce9e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: [['What', 'is', 'the', 'capital', 'city', 'of', 'germany']]\n",
      "Tokenized Output: [['everyone', 'knows', 'that', 'Berlin', 'is', 'Germany', '’', 's', 'Capital', 'right', 'well', 'not', 'exactly', 'The', 'German', 'Parliament', 'building', 'was', 'relocated', 'from', 'Berlin', 'to', 'Bonn', 'in', '1990', 'and', 'remained', 'there', 'until', 'reunification', 'when', 'it', 'moved', 'back', 'to', 'Berlin', 'in', '1999', 'Berlin', 'has', 'been', 'the', 'capital', 'since', 'the', 'country', '’', 's', 'founding', 'in', '1871', 'but', 'for', 'many', 'years', 'it', 'wasn', '’', 't', 'officially', 'designated', 'as', 'such', 'instead', 'being', 'referred', 'to', 'as', 'the', '“', 'Prussian', '”', 'or', '“', 'Imperial', '”', 'capital', 'The', 'history', 'of', 'German', 'government', 'goes', 'back', 'much', 'further', 'than', 'that', 'though', 'and', 'you', 'can', 'see', 'a', 'timeline', 'of', 'how', 'that', 'power', 'evolved', 'and', 'then', 'expanded', 'from', '1495', 'onwards', 'in', 'this', 'infographic', 'It', '’', 's', 'not', 'just', 'the', 'seat', 'of', 'government', 'that', 'has', 'moved', 'around', 'over', 'time', 'either', '–', 'the', 'country', 'itself', 'has', 'grown', 'so', 'big', 'over', 'the', 'centuries', 'that', 'it', 'now', 'consists', 'of', '16', 'states', 'each', 'with', 'their', 'own', 'regional', 'parliament', 'buildings', 'as', 'well']]\n"
     ]
    }
   ],
   "source": [
    "## These lines print the tokenized input and output.\n",
    "print(\"Tokenized Input:\", tokenized_input)\n",
    "print(\"Tokenized Output:\", tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "12625b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags Input: [[('What', 'PRON'), ('is', 'AUX'), ('the', 'DET'), ('capital', 'NOUN'), ('city', 'NOUN'), ('of', 'ADP'), ('germany', 'PROPN')]]\n",
      "POS Tags Output: [[('everyone', 'PRON'), ('knows', 'VERB'), ('that', 'SCONJ'), ('Berlin', 'PROPN'), ('is', 'AUX'), ('Germany', 'PROPN'), ('’', 'PUNCT'), ('s', 'VERB'), ('Capital', 'PROPN'), ('right', 'ADV'), ('well', 'INTJ'), ('not', 'PART'), ('exactly', 'ADV'), ('The', 'DET'), ('German', 'ADJ'), ('Parliament', 'PROPN'), ('building', 'NOUN'), ('was', 'AUX'), ('relocated', 'VERB'), ('from', 'ADP'), ('Berlin', 'PROPN'), ('to', 'ADP'), ('Bonn', 'PROPN'), ('in', 'ADP'), ('1990', 'NUM'), ('and', 'CCONJ'), ('remained', 'VERB'), ('there', 'ADV'), ('until', 'ADP'), ('reunification', 'NOUN'), ('when', 'SCONJ'), ('it', 'PRON'), ('moved', 'VERB'), ('back', 'ADV'), ('to', 'ADP'), ('Berlin', 'PROPN'), ('in', 'ADP'), ('1999', 'NUM'), ('Berlin', 'PROPN'), ('has', 'AUX'), ('been', 'AUX'), ('the', 'DET'), ('capital', 'NOUN'), ('since', 'SCONJ'), ('the', 'DET'), ('country', 'NOUN'), ('’', 'PUNCT'), ('s', 'AUX'), ('founding', 'VERB'), ('in', 'ADP'), ('1871', 'NUM'), ('but', 'CCONJ'), ('for', 'ADP'), ('many', 'ADJ'), ('years', 'NOUN'), ('it', 'PRON'), ('wasn', 'VERB'), ('’', 'PUNCT'), ('t', 'NOUN'), ('officially', 'ADV'), ('designated', 'VERB'), ('as', 'ADP'), ('such', 'ADJ'), ('instead', 'ADV'), ('being', 'AUX'), ('referred', 'VERB'), ('to', 'ADP'), ('as', 'SCONJ'), ('the', 'DET'), ('“', 'PUNCT'), ('Prussian', 'PROPN'), ('”', 'PUNCT'), ('or', 'CCONJ'), ('“', 'PUNCT'), ('Imperial', 'ADJ'), ('”', 'PUNCT'), ('capital', 'NOUN'), ('The', 'DET'), ('history', 'NOUN'), ('of', 'ADP'), ('German', 'ADJ'), ('government', 'NOUN'), ('goes', 'VERB'), ('back', 'ADV'), ('much', 'ADV'), ('further', 'ADV'), ('than', 'ADP'), ('that', 'PRON'), ('though', 'ADV'), ('and', 'CCONJ'), ('you', 'PRON'), ('can', 'AUX'), ('see', 'VERB'), ('a', 'DET'), ('timeline', 'NOUN'), ('of', 'ADP'), ('how', 'SCONJ'), ('that', 'DET'), ('power', 'NOUN'), ('evolved', 'VERB'), ('and', 'CCONJ'), ('then', 'ADV'), ('expanded', 'VERB'), ('from', 'ADP'), ('1495', 'NUM'), ('onwards', 'ADV'), ('in', 'ADP'), ('this', 'DET'), ('infographic', 'NOUN'), ('It', 'PRON'), ('’', 'PUNCT'), ('s', 'VERB'), ('not', 'PART'), ('just', 'ADV'), ('the', 'DET'), ('seat', 'NOUN'), ('of', 'ADP'), ('government', 'NOUN'), ('that', 'PRON'), ('has', 'AUX'), ('moved', 'VERB'), ('around', 'ADV'), ('over', 'ADP'), ('time', 'NOUN'), ('either', 'ADV'), ('–', 'PUNCT'), ('the', 'DET'), ('country', 'NOUN'), ('itself', 'PRON'), ('has', 'AUX'), ('grown', 'VERB'), ('so', 'ADV'), ('big', 'ADJ'), ('over', 'ADP'), ('the', 'DET'), ('centuries', 'NOUN'), ('that', 'SCONJ'), ('it', 'PRON'), ('now', 'ADV'), ('consists', 'VERB'), ('of', 'ADP'), ('16', 'NUM'), ('states', 'NOUN'), ('each', 'PRON'), ('with', 'ADP'), ('their', 'PRON'), ('own', 'ADJ'), ('regional', 'ADJ'), ('parliament', 'NOUN'), ('buildings', 'NOUN'), ('as', 'ADV'), ('well', 'ADV')]]\n"
     ]
    }
   ],
   "source": [
    "## These lines print the part-of-speech tags for the input and output.\n",
    "\n",
    "print(\"POS Tags Input:\", pos_tags_input)\n",
    "print(\"POS Tags Output:\", pos_tags_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "8ecc9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities Input: [[('germany', 'GPE')]]\n",
      "Named Entities Output: [[('Berlin', 'GPE'), ('Germany', 'GPE'), ('German', 'NORP'), ('Parliament', 'ORG'), ('Berlin', 'GPE'), ('Bonn', 'GPE'), ('1990', 'DATE'), ('Berlin', 'GPE'), ('1999', 'DATE'), ('Berlin', 'GPE'), ('1871', 'DATE'), ('many years', 'DATE'), ('Prussian', 'NORP'), ('Imperial', 'ORG'), ('German', 'NORP'), ('1495', 'DATE'), ('the centuries', 'DATE'), ('16', 'CARDINAL')]]\n"
     ]
    }
   ],
   "source": [
    "#These lines print the named entities for the input and output.\n",
    "\n",
    "print(\"Named Entities Input:\", entities_input)\n",
    "print(\"Named Entities Output:\", entities_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "01d5fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Links from Wikipedia and Entity linking: This function processes the entities, fetching Wikipedia URLs and performing additional tasks.\n",
    "# How to Handle the disambiguations?\n",
    "\n",
    "def process_entities(entities):\n",
    "    for sentence_entities in entities:\n",
    "        for entity, label in sentence_entities:\n",
    "            # Fetch Wikipedia URL for each entity\n",
    "            wikipedia_url = fetch_wikipedia_url(entity)\n",
    "            if wikipedia_url:\n",
    "                # Perform candidate entity ranking and further processing\n",
    "                print(f\"Entity: {entity}, Label: {label}\")\n",
    "                print(f\"Wikipedia URL: {wikipedia_url}\\n\")\n",
    "            else:\n",
    "                # Handle unlinkable mention\n",
    "                print(f\"Unlinkable Mention: {entity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6a2d4e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: germany, Label: GPE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Germany\n",
      "\n",
      "Entity: Berlin, Label: GPE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Berlin\n",
      "\n",
      "Entity: Germany, Label: GPE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Germany\n",
      "\n",
      "Entity: German, Label: NORP\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Germany\n",
      "\n",
      "Entity: Parliament, Label: ORG\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Parliament\n",
      "\n",
      "Entity: Berlin, Label: GPE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Berlin\n",
      "\n",
      "Entity: Bonn, Label: GPE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Bon\n",
      "\n",
      "Entity: 1990, Label: DATE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/1999\n",
      "\n",
      "Entity: Berlin, Label: GPE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Berlin\n",
      "\n",
      "Entity: 1999, Label: DATE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/1999\n",
      "\n",
      "Entity: Berlin, Label: GPE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Berlin\n",
      "\n",
      "Entity: 1871, Label: DATE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/1870\n",
      "\n",
      "DisambiguationError: \"After Many Years\" may refer to: \n",
      "After Many Years (1908 film)\n",
      "After Many Years (1930 film)\n",
      "Unlinkable Mention: many years\n",
      "\n",
      "Entity: Prussian, Label: NORP\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Prussia\n",
      "\n",
      "DisambiguationError: \"Imperial\" may refer to: \n",
      "Imperial, California\n",
      "Imperial, Missouri\n",
      "Imperial, Nebraska\n",
      "Imperial, Pennsylvania\n",
      "Imperial, Texas\n",
      "Imperial, West Virginia\n",
      "Imperial, Virginia\n",
      "Imperial County, California\n",
      "Imperial Valley, California\n",
      "Imperial Beach, California\n",
      "Imperial (Madrid)\n",
      "Imperial, Saskatchewan\n",
      "Imperial Apartments\n",
      "Imperial City, Huế\n",
      "Imperial Palace (disambiguation)\n",
      "Imperial Towers\n",
      "The Imperial (Mumbai)\n",
      "Cheritra\n",
      "coach\n",
      "stagecoach\n",
      "moustache\n",
      "Imperial staircase\n",
      "Mutant Chronicles\n",
      "Galactic Empire (Star Wars)\n",
      "Imperial (Elder Scrolls)\n",
      "Imperial (board game)\n",
      "Imperial (book)\n",
      "Imperial (comics)\n",
      "Imperial Records\n",
      "Little Anthony and the Imperials\n",
      "The Imperials\n",
      "Imperial (band)\n",
      "Imperial (Denzel Curry album)\n",
      "Imperial (In Fear and Faith album)\n",
      "Robin Guthrie\n",
      "The Imperial (The Delines album)\n",
      "The Imperial (Flipmode Squad album)\n",
      "Soen\n",
      "Chapter V: Unbent, Unbowed, Unbroken\n",
      "Scobberlotchers\n",
      "Dirty Harriet\n",
      "Alien\n",
      "Imperial (beer)\n",
      "Mint imperial\n",
      "Hotel Imperial\n",
      "Imperial Hotel (disambiguation)\n",
      "The Imperial, New Delhi\n",
      "Chrysler Imperial\n",
      "Imperial (automobile)\n",
      "Imperial (British automobile)\n",
      "Imperial (SP train)\n",
      "Imperial Limited\n",
      "Imperial Air Cargo\n",
      "Imperial Airlines\n",
      "Imperial Airport\n",
      "Imperial Automobile Company\n",
      "Imperial Bank Limited\n",
      "Imperial Oil\n",
      "Imperial Productions\n",
      "Imperial Theater (disambiguation)\n",
      "Imperial Tobacco\n",
      "Imperial Typewriter Company\n",
      "Imperial College London\n",
      "Imperial Academy (Ethiopia)\n",
      "Imperial Valley College\n",
      "Indian Civil Service\n",
      "Imperial Esports\n",
      "Imperials Football Club\n",
      "Imperial Futebol Clube\n",
      "Athena Imperial\n",
      "Barbie Imperial\n",
      "Carlos Eduardo Imperial\n",
      "Carlos R. Imperial\n",
      "Dino Imperial\n",
      "Francisco Imperial\n",
      "paper size\n",
      "wine bottle nomenclature\n",
      "Imperial purple\n",
      "Imperial units\n",
      "Imperial Court (disambiguation)\n",
      "Imperial River (disambiguation)\n",
      "Imperial Seal (disambiguation)\n",
      "Imperial University (disambiguation)\n",
      "Unlinkable Mention: Imperial\n",
      "\n",
      "Entity: German, Label: NORP\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Germany\n",
      "\n",
      "Entity: 1495, Label: DATE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/1395\n",
      "\n",
      "Entity: the centuries, Label: DATE\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/Centuries_(song)\n",
      "\n",
      "Entity: 16, Label: CARDINAL\n",
      "Wikipedia URL: https://en.wikipedia.org/wiki/6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#These lines call the process_entities function for both input and output entities.\n",
    "\n",
    "# Process input entities\n",
    "process_entities(entities_input)\n",
    "\n",
    "# Process output entities\n",
    "process_entities(entities_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "8d7a0e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ['Berlin', 'Germany', 'Capital', 'well', 'Parliament', 'Berlin', 'Bonn', 'Berlin', 'Berlin', 'Prussian']\n"
     ]
    }
   ],
   "source": [
    "def extract_answer(question, llm_output, target_pos_tags=('INTJ', 'PROPN')):\n",
    "    # Check if the question is open or closed\n",
    "    if question.endswith('?'):\n",
    "        # Open question: Extract tokens with specified POS tags as the answer\n",
    "        tokenized_output = preprocessing(llm_output)\n",
    "        pos_tags_output = pos_tagging(tokenized_output)\n",
    "        target_tokens = [token[0] for sentence_pos_tags in pos_tags_output for token in sentence_pos_tags if token[1] in target_pos_tags]\n",
    "        return target_tokens\n",
    "    else:\n",
    "        # Closed question: Extract 'Yes' or 'No' from the output\n",
    "        if 'Yes' in llm_output:\n",
    "            return 'Yes'\n",
    "        elif 'No' in llm_output:\n",
    "            return 'No'\n",
    "        else:\n",
    "            # Fallback to extracting tokens with specified POS tags\n",
    "            tokenized_output = preprocessing(llm_output)\n",
    "            pos_tags_output = pos_tagging(tokenized_output)\n",
    "            target_tokens = [token[0] for sentence_pos_tags in pos_tags_output for token in sentence_pos_tags if token[1] in target_pos_tags]\n",
    "            return target_tokens\n",
    "\n",
    "\n",
    "answer = extract_answer(llm_input, llm_output)\n",
    "print('Answer:', answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c3100bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ['Berlin', 'Germany', 'Capital', 'well', 'Parliament', 'Berlin', 'Bonn', 'Berlin', 'Berlin', 'Prussian']\n"
     ]
    }
   ],
   "source": [
    "def extract_answer(question, llm_output, target_pos_tags=('INTJ', 'PROPN')):\n",
    "    # Check if the question is open or closed\n",
    "    if question.endswith('?'):\n",
    "        # Open question: Extract tokens with specified POS tags as the answer\n",
    "        tokenized_output = preprocessing(llm_output)\n",
    "        pos_tags_output = pos_tagging(tokenized_output)\n",
    "        target_tokens = [token[0] for sentence_pos_tags in pos_tags_output for token in sentence_pos_tags if token[1] in target_pos_tags]\n",
    "        return target_tokens\n",
    "    else:\n",
    "        # Closed question: Extract 'Yes' or 'No' from the output\n",
    "        if 'Yes' in llm_output:\n",
    "            return 'Yes'\n",
    "        elif 'No' in llm_output:\n",
    "            return 'No'\n",
    "        else:\n",
    "            # Fallback to extracting tokens with specified POS tags\n",
    "            tokenized_output = preprocessing(llm_output)\n",
    "            pos_tags_output = pos_tagging(tokenized_output)\n",
    "            target_tokens = [token[0] for sentence_pos_tags in pos_tags_output for token in sentence_pos_tags if token[1] in target_pos_tags]\n",
    "\n",
    "            return target_tokens\n",
    "\n",
    "\n",
    "answer = extract_answer(llm_input, llm_output)\n",
    "print('Answer:', answer)\n",
    "\n",
    "# Print the first word after the sequence \"the answer is\" or \"everybody knows that\"\n",
    "answer_sequence = 'the answer is'\n",
    "answer_sequence2 = 'everybody knows that'\n",
    "index = llm_output.lower().find(answer_sequence)\n",
    "index2 = llm_output.lower().find(answer_sequence2)\n",
    "if index != -1:\n",
    "    words_after_is = llm_output[index + len(answer_sequence):].split()\n",
    "    words_after_that = llm_output[index2 + len(answer_sequence):].split()\n",
    "    if words_after_is:\n",
    "        print(f\"\\nThe answer is: {words_after_is[1]}\")\n",
    "        print(f\"\\nThe answer is: {words_after_that[1]}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4f17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
